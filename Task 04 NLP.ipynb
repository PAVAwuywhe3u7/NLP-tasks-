{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1WNBGZ3XypLTJ3UwufBKFNwNgl13dYvhl","authorship_tag":"ABX9TyP75KNpB5uQslLhp3DA3uHt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v-w3Ksn2-TzM","executionInfo":{"status":"ok","timestamp":1754545599424,"user_tz":-330,"elapsed":43,"user":{"displayName":"TUMMEPALLI PAVANKARTHIK,CSE(2022) Vel Tech, Chennai","userId":"12502267669211204278"}},"outputId":"a8d29a85-0e39-4b64-c04e-a00c46a4ff72"},"outputs":[{"output_type":"stream","name":"stdout","text":["Top 10 frequent words in the document (after removing stopwords):\n","language: 3\n","nlp: 3\n","human: 2\n","machine: 2\n","learning: 2\n","natural: 1\n","processing: 1\n","field: 1\n","artificial: 1\n","intelligence: 1\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}],"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.probability import FreqDist\n","from docx import Document\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('punkt_tab')\n","\n","def load_document(file_path):\n","    doc = Document(file_path)\n","    full_text = []\n","    for para in doc.paragraphs:\n","        full_text.append(para.text)\n","    return '\\n'.join(full_text)\n","\n","def tokenize_document(document):\n","    tokens = word_tokenize(document)\n","    return [word.lower() for word in tokens if word.isalpha()]\n","\n","def remove_stopwords(tokens):\n","    stop_words = set(stopwords.words('english'))\n","    return [word for word in tokens if word not in stop_words]\n","\n","def find_morphology(tokens):\n","    fdist = FreqDist(tokens)\n","    return fdist.most_common(10)\n","\n","\n","document_path = '/content/text-to-word.docx'\n","document = load_document(document_path)\n","\n","tokens = tokenize_document(document)\n","tokens_without_stopwords = remove_stopwords(tokens)\n","morphology = find_morphology(tokens_without_stopwords)\n","\n","\n","print(\"Top 10 frequent words in the document (after removing stopwords):\")\n","for word, frequency in morphology:\n","    print(f\"{word}: {frequency}\")"]}]}